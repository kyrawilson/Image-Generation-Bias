{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8PjM08V6nKMf"
   },
   "outputs": [],
   "source": [
    "#dependencies\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from rembg import remove\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_filepath = '/Users/countrygirl411/Desktop/Image Dataset 2/Kyra'\n",
    "cfd_filepath = '/Users/countrygirl411/Desktop/CFD Version 3.0/Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get tuples of images\n",
    "def sd_filelist():\n",
    "    sd_images = {}\n",
    "    prompts = {}\n",
    "    for file in os.listdir(images_filepath):\n",
    "        if not file.startswith('.'):\n",
    "            num = int(file.split(\"-\")[0])\n",
    "            p = ''.join(file.split(\"-\")[1:]).split(\".\")[0]\n",
    "            if num in sd_images:\n",
    "                sd_images[num].append(file)\n",
    "                if len(p)+1 > len(prompts[num]):\n",
    "                    prompts[num] = p.replace(\"frontfacing\", \"front-facing\")\n",
    "            else:\n",
    "                sd_images[num] = [file]\n",
    "                prompts[num] = p.replace(\"frontfacing\", \"front-facing\")\n",
    "    return sd_images, prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfd_filelist():\n",
    "    cfd = {}\n",
    "    count = 0\n",
    "    for file in os.listdir(cfd_filepath):\n",
    "        if not file.startswith('.'):\n",
    "            trait = file.split(\"-\")[1]\n",
    "            expression = file.split(\"-\")[-1].split(\".\")[0]\n",
    "            if trait+expression not in cfd:\n",
    "                cfd[trait+expression] = [file]\n",
    "            else:\n",
    "                cfd[trait+expression].append(file)\n",
    "    return cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in a single image as numpy array, resizes and crops it\n",
    "def crop_and_resize(img, out_width=512, out_height=512):\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    r = np.ceil(out_height / height)\n",
    "    dim = (int(width * r), out_height)\n",
    "    resized = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
    "    height_resize, width_resize, channels = resized.shape\n",
    "    \n",
    "    crop_min = int((width_resize-out_width)*0.5)\n",
    "    crop_max = int(out_width+(width_resize-out_width)*0.5)\n",
    "    cropped = resized[:, crop_min:crop_max]\n",
    "\n",
    "    #img_array2 = cropped.flatten()\n",
    "    #img_array2 = cropped/255\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CFD images\n",
    "cfd = {}\n",
    "count = 0\n",
    "for file in os.listdir(cfd_filepath):\n",
    "    if not file.startswith('.'):\n",
    "        count += 1\n",
    "        trait = file.split(\"-\")[1]\n",
    "        expression = file.split(\"-\")[-1].split(\".\")[0]\n",
    "\n",
    "        img = cv2.imread(f'{cfd_filepath}/{file}')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        height, width, channels = img.shape\n",
    "\n",
    "        r = 512 / height\n",
    "        dim = (int(width * r), 512)\n",
    "        resized = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
    "        height_resize, width_resize, channels = resized.shape\n",
    "        \n",
    "        crop_min = int((width_resize-512)*0.5)\n",
    "        crop_max = int(512+(width_resize-512)*0.5)\n",
    "        cropped = resized[:, crop_min:crop_max]\n",
    "\n",
    "        #img_array2 = cropped.flatten()\n",
    "        img_array2 = cropped/255\n",
    "\n",
    "        #Checking\n",
    "        #if count in (range(5)):\n",
    "            #i = Image.fromarray(img)\n",
    "            #r = Image.fromarray(resized)\n",
    "            #c = Image.fromarray(cropped)\n",
    "            #i.show()\n",
    "            #r.show()\n",
    "            #c.show()\n",
    "            #print(img_array2)\n",
    "            #print(img_array2)\n",
    "        \n",
    "        if trait+expression not in cfd:\n",
    "            cfd[trait+expression] = [img_array2]\n",
    "        else:\n",
    "            cfd[trait+expression].append(img_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check\n",
    "for key in cfd.keys():\n",
    "    print(f'{key}: {len(cfd[key])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image slicing\n",
    "def cut_images(image_names):\n",
    "    images = []\n",
    "    for image_name in image_names:\n",
    "        img = cv2.imread(f'{images_filepath}/{image_name}')\n",
    "        #img = cv2.imread(image_name)\n",
    "        img2 = img\n",
    "    \n",
    "        height, width, channels = img.shape\n",
    "    \n",
    "        # Number of pieces Horizontally\n",
    "        W_SIZE  = 5\n",
    "        # Number of pieces Vertically to each Horizontal\n",
    "        H_SIZE = 5\n",
    "    \n",
    "        counter = 0\n",
    "\n",
    "        ###NEED TO EXCLUDE ALL BLACK IMAGES\n",
    "    \n",
    "        for ih in range(H_SIZE ):\n",
    "            for iw in range(W_SIZE ):\n",
    "              x = width/W_SIZE * iw\n",
    "              y = height/H_SIZE * ih\n",
    "              h = (height / H_SIZE)\n",
    "              w = (width / W_SIZE )\n",
    "              #print(x,y,h,w)\n",
    "              counter+=1\n",
    "              img = img[int(y):int(y+h), int(x):int(x+w)]\n",
    "              #cv2.imwrite(str(counter) +  \".png\",img)\n",
    "              if not (img.max==0 and img.min==0):\n",
    "                  images.append(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) \n",
    "              img = img2\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "images = cut_images(sd_images[0])\n",
    "print(len(images))\n",
    "print(images[0:5])\n",
    "#for i in range(20):\n",
    "#    img = Image.fromarray(images[i])\n",
    "#    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75uXqBx-EOnF"
   },
   "outputs": [],
   "source": [
    "#Check\n",
    "images_list[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKox8GLhBYyG",
    "outputId": "4756ab81-5230-44f1-cff7-3185ec468770"
   },
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(columns=['prompt_num', 'prompt'].extend(list(cfd.keys())))\n",
    "\n",
    "cos_sim_dict = {'prompt':[], 'prompt_num':[]}\n",
    "\n",
    "for i in range(len(list(sd_images.keys()))):\n",
    "    key = list(sd_images.keys())[i]\n",
    "    images_list = cut_images(sd_images[key])\n",
    "    cos_sim_dict['prompt_num'].append(key)\n",
    "    cos_sim_dict['prompt'].append(prompts[key])\n",
    "    print(prompts[key])\n",
    "        \n",
    "    for cat in cfd.keys():\n",
    "        print(len(cfd[cat]),len(images_list))\n",
    "        cat_score = 0\n",
    "        if cat not in cos_sim_dict:\n",
    "            cos_sim_dict[cat] = []\n",
    "\n",
    "        for img in images_list:\n",
    "            img_array1 = img.flatten()\n",
    "            img_array1 = img_array1/255\n",
    "            \n",
    "            for cfd_img in cfd[cat]:\n",
    "                print(img_array1.shape, cfd_img.shape)\n",
    "                similarity = -1 * (spatial.distance.cosine(img_array1, cfd_img) - 1)\n",
    "                cat_score += similarity\n",
    "                \n",
    "        cat_avg = cat_score/(len(cfd[cat])*len(images_list))\n",
    "        print(cat, cat_avg)\n",
    "        cos_sim_dict[cat].append(cat_avg)\n",
    "\n",
    "    if i%5==0 or i==len(list(sd_images.keys()))-1:\n",
    "        print(cos_sim_dict)\n",
    "        df = pd.DataFrame.from_dict(cos_sim_dict)\n",
    "        df.to_csv(f'CFD_cos_similarity_{i}.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot distributions of scores\n",
    "cos_sim = pd.read_csv('CFD_cos_similarity_20.tsv', sep='\\t')\n",
    "#Have to reshape for better plotting\n",
    "cos_sim = pd.melt(cos_sim, id_vars=['prompt', 'prompt_num'])\n",
    "cos_sim['gender'] = cos_sim['variable'].astype(str).str[1]\n",
    "cos_sim['race'] = cos_sim['variable'].astype(str).str[0]\n",
    "cos_sim['expression'] = cos_sim['variable'].astype(str).str[2:]\n",
    "\n",
    "gplt = sns.stripplot(data=cos_sim, y='prompt', x='value', hue='gender')\n",
    "plt.show()\n",
    "rplt = sns.stripplot(data=cos_sim, y='prompt', x='value', hue='race')\n",
    "plt.show()\n",
    "eplt = sns.stripplot(data=cos_sim, y='prompt', x='value', hue='expression')\n",
    "plt.show()\n",
    "\n",
    "a4_dims = (32, 8)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "allplt = sns.stripplot(data=cos_sim, y='prompt', x='value', ax=ax)\n",
    "for line in range(0,cos_sim.shape[0]):\n",
    "     allplt.text(cos_sim.value[line]+0.0001, cos_sim.prompt[line], \n",
    "     cos_sim.variable[line], horizontalalignment='left', \n",
    "     fontsize=8, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "dim = (64, 64)\n",
    "\n",
    "all_images = []\n",
    "for key in sd_images.keys():\n",
    "    images_list = cut_images(sd_images[key])\n",
    "    images_list = [images_list[i] for i in range(len(images_list)) if i%10==0]\n",
    "    images_list = [cv2.resize(img, dim, interpolation=cv2.INTER_AREA) for img in images_list]\n",
    "    images_list = [img.flatten()/255 for img in images_list]\n",
    "    all_images.extend(images_list)\n",
    "\n",
    "for cat in cfd.keys():\n",
    "    for i in range(len(cfd[cat])):\n",
    "        if i%5==0:\n",
    "            #print(cfd[cat][i].shape)\n",
    "            pic = cv2.resize(cfd[cat][i], dim, interpolation=cv2.INTER_AREA).flatten()\n",
    "            all_images.append(pic)\n",
    "print(len(all_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([len(img) for img in all_images])\n",
    "#print(all_images[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster, decomposition\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "comps = []\n",
    "#print(all_images[0:5])\n",
    "all_images = np.array(all_images)\n",
    "for i in range(10, 101, 10):\n",
    "    print(i)\n",
    "    pca_estimator = decomposition.PCA(n_components=i, svd_solver=\"randomized\", whiten=True)\n",
    "    pca_estimator.fit(all_images)\n",
    "    comps.append(sum(pca_estimator.explained_variance_ratio_))\n",
    "\n",
    "sns.lineplot(x=np.array(list(range(10, 101, 10))), y=np.array(comps))\n",
    "plt.xlabel('PCA components')\n",
    "plt.ylabel('Explained Variance Ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_estimator = decomposition.PCA(n_components=100, svd_solver=\"randomized\", whiten=True)\n",
    "pca_estimator.fit(all_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PCA dimensions\n",
    "dim=(64,64)\n",
    "df = pd.DataFrame(columns=['prompt_num', 'prompt'].extend(list(cfd.keys())))\n",
    "\n",
    "cos_sim_dict = {'prompt':[], 'prompt_num':[]}\n",
    "\n",
    "for i in range(len(list(sd_images.keys()))):\n",
    "    key = list(sd_images.keys())[i]\n",
    "    images_list = cut_images(sd_images[key])\n",
    "    cos_sim_dict['prompt_num'].append(key)\n",
    "    cos_sim_dict['prompt'].append(prompts[key])\n",
    "    print(prompts[key])\n",
    "        \n",
    "    for cat in cfd.keys():\n",
    "        print(len(cfd[cat]),len(images_list))\n",
    "        cat_score = 0\n",
    "        if cat not in cos_sim_dict:\n",
    "            cos_sim_dict[cat] = []\n",
    "\n",
    "        for img in images_list:\n",
    "            img = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
    "            img_array1 = img.flatten()\n",
    "            img_array1 = img_array1/255\n",
    "            img_array1 = pca_estimator.transform(img_array1.reshape(1, -1))\n",
    "            \n",
    "            for cfd_img in cfd[cat]:\n",
    "                cfd_img = cv2.resize(cfd_img, dim, interpolation=cv2.INTER_AREA)\n",
    "                cfd_img = cfd_img.flatten()\n",
    "                cfd_img = pca_estimator.transform(cfd_img.reshape(1, -1))\n",
    "                #print(img_array1, cfd_img)\n",
    "                similarity = -1 * (spatial.distance.cosine(img_array1[0], cfd_img[0]) - 1)\n",
    "                cat_score += similarity\n",
    "                \n",
    "        cat_avg = cat_score/(len(cfd[cat])*len(images_list))\n",
    "        print(cat, cat_avg)\n",
    "        cos_sim_dict[cat].append(cat_avg)\n",
    "\n",
    "    #print(cos_sim_dict)\n",
    "    df = pd.DataFrame.from_dict(cos_sim_dict)\n",
    "    df.to_csv(f'CFD_PCA_cos_similarity.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    #assert len(imgs) == rows*cols\n",
    "\n",
    "    #w, h = imgs[0].size\n",
    "    w,h = (512, 512)\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect and crop face from single image as numpy\n",
    "desktop = \"/Users/countrygirl411/Desktop/\"\n",
    "cascades = [f'{desktop}haarcascade_frontalface_default.xml', f'{desktop}haarcascade_frontalface_alt.xml', f'{desktop}haarcascade_frontalface_alt2.xml', f'{desktop}haarcascade_frontalface_alt_tree.xml', f'{desktop}haarcascade_profileface.xml']\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "def face_detect(img):\n",
    "    #Detect face using open CV\n",
    "    img_cv = img\n",
    "    #Normal image\n",
    "    img_pil = Image.fromarray(img_cv)\n",
    "\n",
    "    #Image with removed background\n",
    "    output = remove(img_pil)\n",
    "    output = np.array(output)\n",
    "    # Convert RGB to BGR\n",
    "    #output = output[:, :, ::-1].copy()\n",
    "\n",
    "    gray_image = cv2.cvtColor(output, cv2.COLOR_BGR2GRAY)\n",
    "    face = []\n",
    "    detected = []\n",
    "    removed = []\n",
    "\n",
    "    #output = remove(img_pil)\n",
    "\n",
    "    for c in cascades:\n",
    "        if len(face)==0:\n",
    "            face_classifier = cv2.CascadeClassifier(c)\n",
    "            #May have to adjust these parameters to avoid false positives\n",
    "            face = face_classifier.detectMultiScale(\n",
    "                gray_image, scaleFactor=1.1, minNeighbors=10, minSize=(int(height*0.05), int(width*0.05))\n",
    "            )\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if len(face) == 0:\n",
    "        detected = img_pil\n",
    "        removed = img_pil\n",
    "        img_pil = img_pil\n",
    "\n",
    "    #Crop to get just face\n",
    "    else:\n",
    "        #for (x, y, w, h) in face: \n",
    "        (x, y, w, h) = face[0]\n",
    "        faces = output[y:y + h, x:x + w]\n",
    "        img_pil = Image.fromarray(faces) #small face with crop and removed\n",
    "        #Remove background\n",
    "        #output = remove(img_pil)\n",
    "        removed = Image.fromarray(output) #original img with background removed\n",
    "        detected = Image.fromarray(cv2.rectangle(img_cv, (x, y), (x+w, y+h), (0, 0, 255), 2)) #original img with box around face\n",
    "    return face, detected, img_pil, removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import image\n",
    "#file = ['5-a front-facing photo of a person who is blind completely.png']\n",
    "#img = cv2.imread(f'{file}')\n",
    "#img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#height, width, channels = img.shape\n",
    "\n",
    "#Calculate amount of white/distribution of color pixels\n",
    "#Compute cosine similarities\n",
    "\n",
    "#Other things that you might want to do:\n",
    "#When to downsample?\n",
    "#How many faces should we take from each image?\n",
    "#Check output of face detection\n",
    "#Save cropped/processed images separately\n",
    "#Check for black images?\n",
    "\n",
    "desktop = \"/Users/countrygirl411/Desktop/\"\n",
    "cascades = [f'{desktop}haarcascade_frontalface_default.xml', f'{desktop}haarcascade_frontalface_alt.xml', f'{desktop}haarcascade_frontalface_alt2.xml', f'{desktop}haarcascade_frontalface_alt_tree.xml', f'{desktop}haarcascade_profileface.xml']\n",
    "\n",
    "faces = {}\n",
    "\n",
    "for j in range(len(list(sd_images.keys()))):\n",
    "    key = list(sd_images.keys())[j]\n",
    "    print(key)\n",
    "    images = cut_images(sd_images[key])\n",
    "    \n",
    "    cropped_images = []\n",
    "    removed_images = []\n",
    "    detected_images = []\n",
    "\n",
    "    faces[key] = []\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        face, cropped, detected_2, removed = face_detect(images[i])\n",
    "        cropped_images.append(cropped)\n",
    "        removed_images.append(removed)\n",
    "        detected_images.append(detected_2)\n",
    "        faces[key].append(face)\n",
    "    \n",
    "    grid1 = image_grid(cropped_images, rows=5, cols=10)\n",
    "    grid1.save(f'{key}_faces_box.png')\n",
    "    grid2 = image_grid(removed_images, rows=5, cols=10)\n",
    "    grid2.save(f'{key}_removed.png')\n",
    "    grid3 = image_grid(detected_images, rows=5, cols=10)\n",
    "    grid3.save(f'{key}_faces.png')\n",
    "    \n",
    "    # Open a file and use dump() \n",
    "    with open('sd_faces.pkl', 'wb') as file: \n",
    "        # A new file will be created \n",
    "        pickle.dump(faces, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop = \"/Users/countrygirl411/Desktop/\"\n",
    "cascades = [f'{desktop}haarcascade_frontalface_default.xml', f'{desktop}haarcascade_frontalface_alt.xml', f'{desktop}haarcascade_frontalface_alt2.xml', f'{desktop}haarcascade_frontalface_alt_tree.xml', f'{desktop}haarcascade_profileface.xml']\n",
    "\n",
    "faces = {}\n",
    "\n",
    "cfd = cfd_filelist()\n",
    "for j in range(len(list(cfd.keys()))):\n",
    "    key = list(cfd.keys())[j]\n",
    "    print(key)\n",
    "\n",
    "    detected_images = []\n",
    "    removed_images = []\n",
    "    detected_2_images = []\n",
    "\n",
    "    for file in cfd[key]:\n",
    "        img = cv2.imread(f'{cfd_filepath}/{file}')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = crop_and_resize(img, out_width=512, out_height=512)\n",
    "        face, detected, detected_2, removed = face_detect(img)\n",
    "\n",
    "        faces[file] = face\n",
    "        detected_images.append(detected)\n",
    "        removed_images.append(removed)\n",
    "        detected_2_images.append(detected_2)\n",
    "\n",
    "    rows = int(np.floor(np.sqrt(len(detected_images))))\n",
    "    cols = int(len(detected_images)/rows + 1)\n",
    "    grid1 = image_grid(detected_images, rows=rows, cols=cols)\n",
    "    grid1.save(f'cfd_{key}_faces_box.png')\n",
    "    grid2 = image_grid(removed_images, rows=rows, cols=cols)\n",
    "    grid2.save(f'cfd_{key}_removed.png')\n",
    "    grid3 = image_grid(detected_2_images, rows=rows, cols=cols)\n",
    "    grid3.save(f'{key}_faces.png')\n",
    "\n",
    "    # Open a file and use dump() \n",
    "    with open('cfd_faces.pkl', 'wb') as file: \n",
    "        # A new file will be created \n",
    "        pickle.dump(faces, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate cosine similarities of detected faces for CFD\n",
    "# open a file, where you stored the pickled data\n",
    "cfd_file = open('/Users/countrygirl411/Desktop/cfd_faces/cfd_faces.pkl', 'rb')\n",
    "sd_file = open('/Users/countrygirl411/Desktop/sd_faces/sd_faces.pkl', 'rb')\n",
    "\n",
    "# dump information to that file\n",
    "cfd_data = pickle.load(cfd_file)\n",
    "sd_data = pickle.load(sd_file)\n",
    "\n",
    "# close the file\n",
    "cfd_file.close()\n",
    "sd_file.close()\n",
    "#In call to cosine similarity... pass in images? crops? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfd_faces():\n",
    "    faces = []\n",
    "    cats = []\n",
    "    count = 1\n",
    "    for cfd_key in cfd_data.keys():\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "        cat = ''.join([cfd_key.split('-')[1], cfd_key.split('-')[-1].split(\".\")[0]])\n",
    "        #print(cfd_key, cat)\n",
    "    \n",
    "        #Load sd_images and cfd_image\n",
    "        cfd_img = cv2.imread(f'{cfd_filepath}/{cfd_key}')\n",
    "        cfd_img = cv2.cvtColor(cfd_img, cv2.COLOR_BGR2RGB)\n",
    "        img_pil = Image.fromarray(cfd_img)\n",
    "        #Image with removed background\n",
    "        cfd_img = remove(img_pil)\n",
    "        cfd_img = np.array(cfd_img)\n",
    "        cfd_img = crop_and_resize(cfd_img, out_width=512, out_height=512)\n",
    "\n",
    "        if len(cfd_data[cfd_key])>0:\n",
    "            (x_c, y_c, w_c, h_c) = cfd_data[cfd_key][0]\n",
    "            cfd_crop = cfd_img[y_c:y_c + h_c, x_c:x_c + w_c]\n",
    "    \n",
    "            trans_mask = cfd_crop[:,:,3] <= 200\n",
    "            cfd_crop[trans_mask] = [127, 127, 127, 127]\n",
    "            cfd_crop = cv2.cvtColor(cfd_crop, cv2.COLOR_BGRA2BGR)\n",
    "            \n",
    "            faces.append(cfd_crop)\n",
    "            cats.append(cat)\n",
    "    return faces, cats\n",
    "\n",
    "cfd_faces_out, cfd_cats_out = cfd_faces()\n",
    "\n",
    "with open('cfd_faces_crop.pkl', 'wb') as file: \n",
    "    # A new file will be created \n",
    "    pickle.dump((cfd_faces_out, cfd_cats_out), file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_images = sd_filelist()[0]\n",
    "#Cos_sim_dict has xxx entries-prompt, prompt_num, and xxx cfd categories\n",
    "#prompt and prompt_num are the length of sd_images\n",
    "#xxx category -> dictionary where the prompt num contains list of similarity scores for an image? \n",
    "cos_sim_dict = {}\n",
    "sd_faces_crop = {}\n",
    "\n",
    "cfd_faces_file = open('/Users/countrygirl411/Desktop/cfd_faces_crop.pkl', 'rb')\n",
    "# dump information to that file\n",
    "cfd_faces_out, cfd_cats_ou = pickle.load(cfd_faces_file)\n",
    "# close the file\n",
    "cfd_faces_file.close()\n",
    "\n",
    "keys = list(sd_data.keys())\n",
    "keys.sort()\n",
    "\n",
    "for sd_key in keys:\n",
    "    print(int(sd_key))\n",
    "    #cos_sim_dict['prompt_num'].append(sd_key)\n",
    "    #cos_sim_dict['prompt'].append(prompts[sd_key])\n",
    "    cos_sim_dict[sd_key] = {}\n",
    "    sd_faces_crop[sd_key] = []\n",
    "    sd_imgs = cut_images(sd_images[int(sd_key)])\n",
    "\n",
    "    for i in range(len(sd_imgs)):\n",
    "        sd_img = Image.fromarray(sd_imgs[i])\n",
    "        #Image with removed background\n",
    "        sd_img = remove(sd_img)\n",
    "        sd_img = np.array(sd_img)\n",
    "\n",
    "        # Threshold-> can be greater than zero. eg 0.006 //Grayscale\n",
    "        gray = (np.abs(sd_img[0]-sd_img[1]) < 11).all() and (np.abs(sd_img[0]-sd_img[2])< 11).all() and (np.abs(sd_img[1]-sd_img[2])< 11).all()\n",
    "        #Crop faces / remove background\n",
    "        sd_faces = sd_data[sd_key][i]\n",
    "        if len(sd_faces) > 0 and not gray:\n",
    "            (x, y, w, h) = sd_faces[0]\n",
    "            sd_crop = sd_img[y:y + h, x:x + w]\n",
    "            trans_mask = sd_crop[:,:,3] <= 200\n",
    "            sd_crop[trans_mask] = [127, 127, 127, 127]\n",
    "            sd_crop = cv2.cvtColor(sd_crop, cv2.COLOR_BGRA2BGR)\n",
    "            sd_faces_crop[sd_key].append(sd_crop)\n",
    "            \n",
    "            for j in range(len(cfd_faces_out)):\n",
    "                cfd_crop = cfd_faces_out[j]\n",
    "                cat = cfd_cats_out[j]\n",
    "                #print(sd_crop.shape, cfd_crop.shape)\n",
    "\n",
    "                if sd_crop.shape[0] > cfd_crop.shape[0]:\n",
    "                    sd_crop = crop_and_resize(sd_crop, cfd_crop.shape[0], cfd_crop.shape[0])\n",
    "                else:\n",
    "                    cfd_crop = crop_and_resize(cfd_crop, sd_crop.shape[0], sd_crop.shape[0])\n",
    "                #print(sd_crop.shape, cfd_crop.shape)\n",
    "\n",
    "                #Compute RGB cosine similarity\n",
    "                sd_flat = sd_crop.flatten()/255\n",
    "                cfd_flat = cfd_crop.flatten()/255\n",
    "                #print(sd_flat.shape, cfd_flat.shape)\n",
    "                similarity = -1 * (spatial.distance.cosine(sd_flat, cfd_flat) - 1)\n",
    "\n",
    "                if cat in cos_sim_dict[sd_key]:\n",
    "                    cos_sim_dict[sd_key][cat].append(similarity)\n",
    "                else:\n",
    "                    cos_sim_dict[sd_key][cat] = [similarity]    \n",
    "\n",
    "                #print(cos_sim_dict)\n",
    "    \n",
    "    with open('cos_sim_dict.pkl', 'wb') as file: \n",
    "        # A new file will be created \n",
    "        pickle.dump(cos_sim_dict, file)\n",
    "        file.close()\n",
    "\n",
    "    with open('sd_faces_crop.pkl', 'wb') as file: \n",
    "        # A new file will be created \n",
    "        pickle.dump(sd_faces_crop, file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_images = sd_filelist()[0]\n",
    "#Cos_sim_dict has xxx entries-prompt, prompt_num, and xxx cfd categories\n",
    "#prompt and prompt_num are the length of sd_images\n",
    "#xxx category -> dictionary where the prompt num contains list of similarity scores for an image? \n",
    "cos_sim_dict = {}\n",
    "sd_faces_crop = {}\n",
    "\n",
    "cfd_faces_file = open('/Users/countrygirl411/Desktop/cfd_faces_crop.pkl', 'rb')\n",
    "# dump information to that file\n",
    "cfd_faces_out, cfd_cats_out = pickle.load(cfd_faces_file)\n",
    "# close the file\n",
    "cfd_faces_file.close()\n",
    "\n",
    "sd_faces_file = open('/Users/countrygirl411/Desktop/sd_faces_crop.pkl', 'rb')\n",
    "sd_faces_out = pickle.load(sd_faces_file)\n",
    "# close the file\n",
    "sd_faces_file.close()\n",
    "\n",
    "keys = list(sd_faces_out.keys())\n",
    "keys.sort()\n",
    "\n",
    "for sd_key in keys:\n",
    "    print(int(sd_key))\n",
    "    cos_sim_dict[sd_key] = {}\n",
    "    \n",
    "    for i in range(len(sd_faces_out[sd_key])):\n",
    "        sd_img = sd_faces_out[sd_key][i]\n",
    "        gray = (np.abs(sd_img[0]-sd_img[1]) < 11).all() and (np.abs(sd_img[0]-sd_img[2])< 11).all() and (np.abs(sd_img[1]-sd_img[2])< 11).all()\n",
    "        \n",
    "        if not gray:\n",
    "            for j in range(len(cfd_faces_out)):\n",
    "                cfd_crop = cfd_faces_out[j]\n",
    "                cat = cfd_cats_out[j]\n",
    "                sd_crop = sd_img\n",
    "                #print(sd_crop.shape, cfd_crop.shape)\n",
    "\n",
    "                if sd_crop.shape[0] > cfd_crop.shape[0]:\n",
    "                    sd_crop = crop_and_resize(sd_crop, cfd_crop.shape[0], cfd_crop.shape[0])\n",
    "                else:\n",
    "                    cfd_crop = crop_and_resize(cfd_crop, sd_crop.shape[0], sd_crop.shape[0])\n",
    "                #print(sd_crop.shape, cfd_crop.shape)\n",
    "\n",
    "                #Compute RGB cosine similarity\n",
    "                sd_flat = sd_crop.flatten()/255\n",
    "                cfd_flat = cfd_crop.flatten()/255\n",
    "                #print(sd_flat.shape, cfd_flat.shape)\n",
    "                similarity = -1 * (spatial.distance.cosine(sd_flat, cfd_flat) - 1)\n",
    "\n",
    "                if cat in cos_sim_dict[sd_key]:\n",
    "                    cos_sim_dict[sd_key][cat].append(similarity)\n",
    "                else:\n",
    "                    cos_sim_dict[sd_key][cat] = [similarity]   \n",
    "\n",
    "    with open('cos_sim_dict.pkl', 'wb') as file: \n",
    "        # A new file will be created \n",
    "        pickle.dump(cos_sim_dict, file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to count faces in sd_data -> that can all be done in a separate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster, decomposition\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cos_sim_dict_avg = {}\n",
    "\n",
    "for key in cos_sim_dict.keys():\n",
    "    cos_sim_dict_avg[key] = {}\n",
    "    for key_2 in cos_sim_dict[key]:\n",
    "        cos_sim_dict_avg[key][key_2] = np.mean(cos_sim_dict[key][key_2])\n",
    "\n",
    "cos_sim_df = pd.DataFrame.from_dict(cos_sim_dict_avg, orient='index')\n",
    "cos_sim_df['prompt'] = prompts\n",
    "cos_sim_df.to_csv(f'cos_similarity_cropped.tsv', sep='\\t', index=False)\n",
    "\n",
    "cos_sim = pd.melt(cos_sim_df, id_vars=['prompt'])\n",
    "cos_sim['gender'] = cos_sim['variable'].astype(str).str[1]\n",
    "cos_sim['race'] = cos_sim['variable'].astype(str).str[0]\n",
    "cos_sim['expression'] = cos_sim['variable'].astype(str).str[2:]\n",
    "\n",
    "cos_sim.to_csv(f'cos_similarity_cropped_melted.tsv', sep='\\t', index=False)\n",
    "\n",
    "a4_dims = (32, 16)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "gplt = sns.stripplot(data=cos_sim[0:500], y='prompt', x='value', hue='gender')\n",
    "plt.show()\n",
    "\n",
    "a4_dims = (32, 16)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "rplt = sns.stripplot(data=cos_sim[0:500], y='prompt', x='value', hue='race')\n",
    "plt.show()\n",
    "\n",
    "a4_dims = (32, 16)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "eplt = sns.stripplot(data=cos_sim[0:500], y='prompt', x='value', hue='expression')\n",
    "plt.show()\n",
    "\n",
    "a4_dims = (32, 16)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "allplt = sns.stripplot(data=cos_sim[0:500], y='prompt', x='value', ax=ax)\n",
    "for line in range(0,cos_sim[0:500].shape[0]):\n",
    "     allplt.text(cos_sim[0:500].value[line]+0.0001, cos_sim[0:500].prompt[line], \n",
    "     cos_sim[0:500].variable[line], horizontalalignment='left', \n",
    "     fontsize=8, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLIP cosine similarity\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "cfd_faces_file = open('/Users/countrygirl411/Desktop/cfd_faces_crop.pkl', 'rb')\n",
    "cfd_faces_out, cfd_cats_out = pickle.load(cfd_faces_file)\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "face_clip_features = []\n",
    "\n",
    "for f in cfd_faces_out:\n",
    "    img = Image.fromarray(f)\n",
    "    inputs = processor(images=img, return_tensors=\"pt\")\n",
    "    image_features = model.get_image_features(**inputs)\n",
    "    #print(f.shape, image_features.shape)\n",
    "    face_clip_features.append(image_features.detach().numpy())\n",
    "\n",
    "with open('cfd_clip_features.pkl', 'wb') as file: \n",
    "    # A new file will be created \n",
    "    pickle.dump((cfd_cats_out, face_clip_features), file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_faces_file = open('/Users/countrygirl411/Desktop/sd_faces_crop.pkl', 'rb')\n",
    "sd_faces_out = pickle.load(sd_faces_file)\n",
    "sd_faces_file.close()\n",
    "\n",
    "keys = list(sd_faces_out.keys())\n",
    "keys.sort()\n",
    "\n",
    "face_clip_features = {}\n",
    "\n",
    "for sd_key in keys:\n",
    "    print(sd_key)\n",
    "    face_clip_features[sd_key] = []\n",
    "    for f in sd_faces_out[sd_key]:\n",
    "        img = Image.fromarray(f)\n",
    "        inputs = processor(images=img, return_tensors=\"pt\")\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "        #print(f.shape, image_features.shape)\n",
    "        face_clip_features[sd_key].append(image_features.detach().numpy())\n",
    "\n",
    "with open('sd_clip_features.pkl', 'wb') as file: \n",
    "    # A new file will be created \n",
    "    pickle.dump(face_clip_features, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd_clip_file = open('/Users/countrygirl411/Desktop/cfd_clip_features.pkl', 'rb')\n",
    "# dump information to that file\n",
    "cfd_cats_out, cfd_clip_out = pickle.load(cfd_clip_file)\n",
    "# close the file\n",
    "cfd_clip_file.close()\n",
    "\n",
    "sd_clip_file = open('/Users/countrygirl411/Desktop/sd_clip_features.pkl', 'rb')\n",
    "sd_clip_out = pickle.load(sd_clip_file)\n",
    "# close the file\n",
    "sd_clip_file.close()\n",
    "\n",
    "keys = list(sd_clip_out.keys())\n",
    "keys.sort()\n",
    "cos_sim_dict = {}\n",
    "\n",
    "for sd_key in keys:\n",
    "    print(int(sd_key))\n",
    "    cos_sim_dict[sd_key] = {}\n",
    "\n",
    "    for i in range(len(sd_clip_out[sd_key])):\n",
    "        sd_img = sd_clip_out[sd_key][i]\n",
    "\n",
    "        #turn into features so we don't know if img is gray or not at this pt \n",
    "        #gray = (np.abs(sd_img[0]-sd_img[1]) < 11).all() and (np.abs(sd_img[0]-sd_img[2])< 11).all() and (np.abs(sd_img[1]-sd_img[2])< 11).all()\n",
    "        #print(gray)\n",
    "        #if not gray:\n",
    "        \n",
    "        for j in range(len(cfd_clip_out)):\n",
    "            cfd_crop = cfd_clip_out[j].squeeze()\n",
    "            cat = cfd_cats_out[j]\n",
    "            sd_crop = sd_img.squeeze()\n",
    "            #print(cfd_crop.shape, sd_crop.shape)\n",
    "            #print(sd_crop.shape, cfd_crop.shape)\n",
    "\n",
    "            #if sd_crop.shape[0] > cfd_crop.shape[0]:\n",
    "                #sd_crop = crop_and_resize(sd_crop, cfd_crop.shape[0], cfd_crop.shape[0])\n",
    "            #else:\n",
    "                #cfd_crop = crop_and_resize(cfd_crop, sd_crop.shape[0], sd_crop.shape[0])\n",
    "            #print(sd_crop.shape, cfd_crop.shape)\n",
    "\n",
    "            #Compute RGB cosine similarity\n",
    "            #sd_flat = sd_crop.flatten()/255\n",
    "            #cfd_flat = cfd_crop.flatten()/255\n",
    "            #print(sd_flat.shape, cfd_flat.shape)\n",
    "            similarity = -1 * (spatial.distance.cosine(sd_crop, cfd_crop) - 1)\n",
    "\n",
    "            if cat in cos_sim_dict[sd_key]:\n",
    "                cos_sim_dict[sd_key][cat].append(similarity)\n",
    "            else:\n",
    "                cos_sim_dict[sd_key][cat] = [similarity]\n",
    "\n",
    "    with open('clip_cos_sim_dict.pkl', 'wb') as file: \n",
    "        # A new file will be created \n",
    "        pickle.dump(cos_sim_dict, file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster, decomposition\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sd_images, prompts = sd_filelist()\n",
    "cos_sim_dict_avg = {}\n",
    "\n",
    "for key in cos_sim_dict.keys():\n",
    "    cos_sim_dict_avg[key] = {}\n",
    "    for key_2 in cos_sim_dict[key]:\n",
    "        cos_sim_dict_avg[key][key_2] = np.mean(cos_sim_dict[key][key_2])\n",
    "\n",
    "cos_sim_df = pd.DataFrame.from_dict(cos_sim_dict_avg, orient='index')\n",
    "cos_sim_df['prompt'] = prompts\n",
    "cos_sim_df.to_csv(f'clip_cos_similarity_cropped.tsv', sep='\\t', index=False)\n",
    "\n",
    "cos_sim = pd.melt(cos_sim_df, id_vars=['prompt'])\n",
    "cos_sim['gender'] = cos_sim['variable'].astype(str).str[1]\n",
    "cos_sim['race'] = cos_sim['variable'].astype(str).str[0]\n",
    "cos_sim['expression'] = cos_sim['variable'].astype(str).str[2:]\n",
    "\n",
    "cos_sim.to_csv(f'clip_cos_similarity_cropped_melted.tsv', sep='\\t', index=False)\n",
    "\n",
    "a4_dims = (25, 16)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "gplt = sns.stripplot(data=cos_sim[0:500], y='prompt', x='value', hue='gender')\n",
    "plt.show()\n",
    "\n",
    "a4_dims = (25, 16)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "rplt = sns.stripplot(data=cos_sim[0:500], y='prompt', x='value', hue='race')\n",
    "plt.show()\n",
    "\n",
    "a4_dims = (25, 16)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "eplt = sns.stripplot(data=cos_sim[0:500], y='prompt', x='value', hue='expression')\n",
    "plt.show()\n",
    "\n",
    "a4_dims = (32, 16)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "allplt = sns.stripplot(data=cos_sim[0:500], y='prompt', x='value', ax=ax)\n",
    "for line in range(0,cos_sim[0:500].shape[0]):\n",
    "     allplt.text(cos_sim[0:500].value[line]+0.0001, cos_sim[0:500].prompt[line], \n",
    "     cos_sim[0:500].variable[line], horizontalalignment='left', \n",
    "     fontsize=8, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measuring prompt felicity: A) size of subject, B) depiction of other objects, \n",
    "#C) deviation of background from plain, D) coloration of the image, E) difference in gender/race ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For baseline images, cut images, remove background, crop faces, save cropped faces\n",
    "\n",
    "crops = []\n",
    "positions = []\n",
    "baseline_path = '/Users/countrygirl411/Desktop/template images'\n",
    "for file in os.listdir(baseline_path):\n",
    "    print(file)\n",
    "    imgs = cut_images([f'{baseline_path}/{file}'])\n",
    "    for i in imgs:\n",
    "        face, detected, img_pil, removed = face_detect(i)\n",
    "        positions.append(face)\n",
    "        crops.append(img_pil)\n",
    "\n",
    "with open('baseline_crop.pkl', 'wb') as file: \n",
    "    # A new file will be created \n",
    "    pickle.dump(crops, file)\n",
    "    file.close()\n",
    "\n",
    "with open('baseline_positions.pkl', 'wb') as file: \n",
    "    # A new file will be created \n",
    "    pickle.dump(positions, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline sizes\n",
    "\n",
    "baseline_faces_file = open('baseline_positions.pkl', 'rb')\n",
    "# dump information to that file\n",
    "b_faces = pickle.load(baseline_faces_file)\n",
    "# close the file\n",
    "baseline_faces_file.close()\n",
    "\n",
    "sizes = []\n",
    "for f in b_faces:\n",
    "    if len(f) > 0: #(x, y, w, h)\n",
    "        s = f[0][2] * f[0][3]\n",
    "        s = s / 512**2\n",
    "        sizes.append(s)\n",
    "\n",
    "print(\"Mean: \", np.mean(sizes))\n",
    "print(\"STD: \", np.std(sizes))\n",
    "print(len(sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt sizes\n",
    "sd_faces_file = open('/Users/countrygirl411/Desktop/sd_faces/sd_faces.pkl', 'rb')\n",
    "# dump information to that file\n",
    "sd_faces = pickle.load(sd_faces_file)\n",
    "# close the file\n",
    "sd_faces_file.close()\n",
    "\n",
    "keys = list(sd_faces.keys())\n",
    "keys.sort()\n",
    "\n",
    "face_size_dict = {'mean': [], 'std': [], 'prompts': [], 'prompt_num': list(range(93)), 'len': []}\n",
    "\n",
    "for k in keys:\n",
    "    sizes = []\n",
    "    for f in sd_faces[k]:\n",
    "        if len(f) > 0: #(x, y, w, h)\n",
    "            s = f[0][2] * f[0][3]\n",
    "            s = s / 512**2\n",
    "            sizes.append(s)\n",
    "    face_size_dict['mean'].append(np.mean(sizes))\n",
    "    face_size_dict['std'].append(np.std(sizes))\n",
    "    face_size_dict['len'].append(len(sizes))\n",
    "    face_size_dict['prompts'].append(prompts[k])\n",
    "\n",
    "df = pd.DataFrame(face_size_dict)\n",
    "df.to_csv('sd_face_sizes.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coloration-maybe only look at gray for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline object detection\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "objects = []\n",
    "\n",
    "baseline_path = '/Users/countrygirl411/Desktop/template images'\n",
    "for file in os.listdir(baseline_path):\n",
    "    print(file)\n",
    "    imgs = cut_images([f'{baseline_path}/{file}'])\n",
    "    for i in imgs:\n",
    "        img_pil = Image.fromarray(i)\n",
    "        results = model.predict(img_pil)\n",
    "        objects.append(results)\n",
    "        #print(results)\n",
    "\n",
    "with open('baseline_objects.pkl', 'wb') as file: \n",
    "    # A new file will be created \n",
    "    pickle.dump(objects, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SD object detection\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "imgs, prompts = sd_filelist()\n",
    "keys = list(imgs.keys())\n",
    "keys.sort()\n",
    "\n",
    "objects = {}\n",
    "\n",
    "for k in keys:\n",
    "    print(k)\n",
    "    objects[k] = []\n",
    "    images = cut_images(imgs[k])\n",
    "    for i in images:\n",
    "        out = []\n",
    "        img_pil = Image.fromarray(i)\n",
    "        results = model.predict(img_pil, verbose=False)\n",
    "        for res in results:\n",
    "            out.append(res.boxes)\n",
    "        objects[k].append(out)\n",
    "\n",
    "with open('sd_objects.pkl', 'wb') as file: \n",
    "    # A new file will be created \n",
    "    pickle.dump(objects, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "{'person': 535, 'car': 15, 'tie': 13, 'dog': 1, 'tennis racket': 1, 'cat': 1, 'donut': 2, 'traffic light': 2, 'tv': 1}\n"
     ]
    }
   ],
   "source": [
    "#Count baseline objects\n",
    "\n",
    "base_obj_file = open('baseline_objects.pkl', 'rb')\n",
    "# dump information to that file\n",
    "base_obj = pickle.load(base_obj_file)\n",
    "# close the file\n",
    "base_obj_file.close()\n",
    "\n",
    "counts = {}\n",
    "\n",
    "print(len(base_obj))\n",
    "for b in base_obj:\n",
    "    names = b[0].names\n",
    "    boxes = b[0].boxes\n",
    "    cls = boxes.cls.numpy()\n",
    "    for c in cls:\n",
    "        if names[int(c)] not in counts:\n",
    "            counts[names[int(c)]] = 1\n",
    "        else:\n",
    "            counts[names[int(c)]] += 1\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count SD objects\n",
    "imgs, prompts = sd_filelist()\n",
    "\n",
    "sd_obj_file = open('sd_objects.pkl', 'rb')\n",
    "# dump information to that file\n",
    "sd_obj = pickle.load(sd_obj_file)\n",
    "# close the file\n",
    "sd_obj_file.close()\n",
    "\n",
    "counts = {}\n",
    "\n",
    "for k in list(sd_obj.keys()):\n",
    "    for b in sd_obj[k]:\n",
    "        box = b[0]\n",
    "        cls = b[0].cls.numpy()\n",
    "        for c in cls:\n",
    "            if names[int(c)] not in counts:\n",
    "                counts[names[int(c)]] = [0 for i in range(93)]\n",
    "            counts[names[int(c)]][k] += 1\n",
    "\n",
    "counts['prompt_num'] = list(range(93))\n",
    "counts['prompt'] = [prompts[p] for p in counts['prompt_num']]\n",
    "df = pd.DataFrame(counts)\n",
    "df['total'] = df.drop(columns=['prompt_num', 'person']).sum(axis=1, numeric_only=True)\n",
    "\n",
    "df.to_csv('sd_obj_counts.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080 1120\n",
      "2184 1176\n",
      "20 21\n",
      "104.0 56.0\n",
      "104.0 56.0\n"
     ]
    }
   ],
   "source": [
    "#Get individual race/gender breakdowns for each prompt\n",
    "#For each prompt, go through each image and compare with cfd images\n",
    "#For each prompt, save face no. and similarity with each of the categories\n",
    "#Try not to kill yourself\n",
    "\n",
    "cos_sim_dict_file = open('clip_cos_sim_dict.pkl', 'rb')\n",
    "# dump information to that file\n",
    "cos_sim_dict = pickle.load(cos_sim_dict_file)\n",
    "# close the file\n",
    "cos_sim_dict_file.close()\n",
    "\n",
    "cfd_file = open('cfd_clip_features.pkl', 'rb')\n",
    "# dump information to that file\n",
    "cfd_faces = pickle.load(cfd_file)\n",
    "# close the file\n",
    "cfd_file.close()\n",
    "\n",
    "sd_file = open('sd_clip_features.pkl', 'rb')\n",
    "# dump information to that file\n",
    "sd_faces = pickle.load(sd_file)\n",
    "# close the file\n",
    "sd_file.close()\n",
    "\n",
    "#print(sd_faces)\n",
    "#print(cfd_faces[1])\n",
    "\n",
    "\n",
    "#How the fuck did these numbers even workkkk\n",
    "#This is why I did averages\n",
    "#I guess it's probably for each image, similarity with each face and then all in the same list\n",
    "#So can figure out individual image breakdowns by looking at specific chunks of the list? \n",
    "print(len(cos_sim_dict[1]['BFN']), len(cos_sim_dict[1]['LFN']))\n",
    "print(len(cos_sim_dict[2]['BFN']), len(cos_sim_dict[2]['LFN']))\n",
    "print(len(sd_faces[1]), len(sd_faces[2]))\n",
    "print(len(cos_sim_dict[1]['BFN'])/len(sd_faces[1]), len(cos_sim_dict[1]['LFN'])/len(sd_faces[1]))\n",
    "print(len(cos_sim_dict[2]['BFN'])/len(sd_faces[2]), len(cos_sim_dict[2]['LFN'])/len(sd_faces[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarities loop through each image, and then each CFD images within that\n",
    "#For each prompt\n",
    "#Need how many images are of each race/gender--for each image, get category of CFD image that has the highest similarity\n",
    "keys = list(cos_sim_dict.keys())\n",
    "keys.sort()\n",
    "\n",
    "max_dict = {}\n",
    "for key in keys:\n",
    "    cats = list(cos_sim_dict[key].keys())\n",
    "    max_dict[key] = {}\n",
    "    for cat in cats:\n",
    "        prompt_len = len(cos_sim_dict[key][cat])/len(sd_faces[key])\n",
    "        max_dict[key][cat] = []\n",
    "        for i in range(len(cos_sim_dict[key][cat])):\n",
    "            if i % prompt_len == 0:\n",
    "                max_sim = 0\n",
    "            sim = cos_sim_dict[key][cat][i]\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "            if (i+1) % prompt_len == 0:\n",
    "                #print(\"True\")\n",
    "                max_dict[key][cat].append(max_sim)\n",
    "#print(max_dict)\n",
    "\n",
    "results = {}\n",
    "for key in keys:\n",
    "    results[key] = []\n",
    "    for i in range(len(sd_faces[key])):\n",
    "        max_sim = 0\n",
    "        max_cat = ''\n",
    "        for cat in max_dict[key].keys():\n",
    "            sim = max_dict[key][cat][i]\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                max_cat = cat\n",
    "        results[key].append(max_cat)\n",
    "\n",
    "#print(results)\n",
    "out_dict = {}\n",
    "for key in keys:\n",
    "    r_list = [f'{r[0]}_race' for r in results[key]]\n",
    "    race = {x:r_list.count(x) for x in r_list}\n",
    "    g_list = [f'{r[1]}_gender' for r in results[key]]\n",
    "    gender = {x:g_list.count(x) for x in g_list}\n",
    "    e_list = [f'{r[2:]}_expression' for r in results[key]]\n",
    "    expression = {x:e_list.count(x) for x in e_list}\n",
    "    r = {**race, **gender, **expression}\n",
    "    out_dict[key] = r\n",
    "    \n",
    "#print(out_dict)\n",
    "out_df = pd.DataFrame.from_dict(out_dict, orient='index')\n",
    "out_df['prompt_num'] = list(range(93))\n",
    "out_df['prompt'] = [prompts[p] for p in counts['prompt_num']]\n",
    "#print(out_df)\n",
    "out_df.to_csv('cdc_comparison_counts.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
